# Literature {#litreview}

In a groundbreaking theoretical article, @berdica2002 attempted to identify,
define and conceptualize network "vulnerability" --- the complement of 
resiliency --- by envisioning analyses conducted with
several vulnerability performance measures including travel time, delay,
congestion, serviceability, and accessibility. She then defined reliability as
the level of reduced accessibility due to unfavorable operating conditions on
the network. In particular, Berdica identified a need for further research
toward developing a framework capable of investigating reliability of
transportation networks. 

In this section we examine several attempts by numerous researchers to do
precisely this using various measures of network performance. It is helpful to
categorize this review into three groups based on the overall technique applied 
in the study:

-	*Network connectivity*: How does damage to a network diminish the connectivity 
between network nodes?

-	*Travel time analysis*: How much do shortest path travel times between origins 
and destinations increase on a damaged network?

-	*Accessibility analysis* How easily can the population using the damaged
network complete their daily activities?

The following sections discuss relevant studies in each group; Table
\@ref(tab:authortable) consolidates these studies by year and labeled with
applicable group.

```{r authortable, echo = FALSE}
read_xlsx("images/author_summary.xlsx") %>%
  kbl(caption = "Attempts to Evaluate Systemic Resiliency", booktabs = TRUE) %>%
  kable_styling()
```

## Network Connectivity
The purpose of transportation networks is to connect locations to each other; 
presumably damage to a network would diminish the network's *connectivity*, or 
the number of paths between node pairs. It may even leave nodes or groups of nodes
completely isolated. In studies based on connectivity, researchers typically apply
methods and concepts from graph theory.

@abdel2007 developed a multi-layered graph to examine the resiliency of the traffic
signal control system in Boise, Idaho. The researchers determined which 
traffic signals would be isolated by a failure to a particular power substation,
and consequentially the percent of travel paths that would experience diminished 
levels of service. The research highlights the degree to which interrelated
infrastructure systems --- power, telecommunications, and transportation ---
depend on each other, though the researchers did not attempt to look at the
connective resiliency of the transportation network directly.

@agarwal2011 present a method to represent a transportation network as a 
hierarchical graph that can be analyzed more directly for vulnerabilities. The
authors acknowledge, however, that a maximal failure consideration where a node
is entirely isolated from the network is unlikely in a real-world network with 
multiple paths of connectivity.

@ip2011 address this shortcoming through the concept of *friability*, or
the reduction of capacity cased by removing a link or node, in order to
determine criticality of individual links. The methodology relies on the ability
to determine the weighted sum of the resilience of all nodes based on the
weighted average of connectedness with other city nodes in the network. The
authors determine that the recovery of transportability between two cities
largely depends on redundant links between nodes. The authors also comment that
most traffic managers are more concerned with the friability of single links
rather than the friability of multiple links or an entire system.

@vodak2019 develop an approach to identify
critical links in a network by searching for the shortest independent
loops in the network. The algorithm progressively damages one or more links
between iterations to determine if nodes become isolated, or cut off from the
network. If a node becomes easily isolated or has a higher likelihood of
becoming isolated, then there is a higher degree of vulnerability present in the
network. This method can both identify critical links in individual networks, as
well as provide a means to quantitatively compare networks.

All of these graph theoretical approaches tend to break down to some degree on
large, real-world networks where the number of nodes and links numbers in the tens
of thousands, and the degree of connectivity between any node pair is high. 
Highway system network failures --- in most imaginable cases --- degrade the
shortest or least cost path but typically do not eliminate it entirely.

## Changes in Travel Time
Graph theory based approaches to resiliency largely consider whether the network
nodes become isolated after links are degraded or removed. Though isolation is
an important problem, transportation networks often have multiple redundant
paths between nodes; however, some of these paths may be considerably longer. It
is therefore useful to determine how the path cost changes even if the nodes
involved are still reachable on the network.

[@peeta2010] constructs a model to evaluate the most efficient allocation of
highway maintenance resources prior to an earthquake with the potential to
disrupt transport network connectivity. The authors create a simplified graph of
Istanbul, Turkey with 30 attributed links and 25 nodes, with each link having a
specified failure probability that could be mitigated with investment. Using
this graph, the researchers evaluate what happened to travel times between the
origin-destination pairs as links from this network were disabled using a Monte
Carlo simulation. The authors showed that this problem is tractable with a
locally optimal solution existing.

[@ibrahim2011] provides an alternative heuristic approach for determining
vulnerability of infrastructure by estimating the cost of single link failure
based on the increase in shortest path travel time due to increased congestion
levels. Ibrahim proposes a hybrid heuristic approach that calculates the
traditional user-equilibrium assignment for finding the first set of costs, and
then fixes those costs for all following iterations to determine the effects of
failure on overall travel time of the system. Ibrahim’s novel heuristic approach
is important because of its ability to drastically reduce computation time for
larger networks while providing accurate results that closely match results
found from traditional modeling methods.

[@omer2013] proposes a methodology for assessing the resiliency of physical
infrastructure during disruptions. To do this, the authors use a network model
to build an origin-destination matrix that allows initial network loading and
analysis. Omer’s model uses several metrics, but the main metric used to
determine resiliency is the difference in travel time between a disturbed and
undisturbed network. Omer’s framework is applied to an actual network between
New York City and Boston for analysis. Changes in demand, travel time, mode
choice and route choice are tracked for analysis. Omer’s framework supports
operability of transportation networks due to the way it analyzes networks
experiencing suboptimal circumstances. The authors work identifies key
parameters that should be measured to assess resiliency during disruptive
events.

[@jaller2015] seeks to identify critical infrastructure based on increased
travel time, or reduced capacity due to disaster. The proposed methodology
utilizes user-equilibrium to determine proper initial network loading. Then, the
shortest path between one origin and one destination can be identified. To
implement damage to the network, a link is cut, and then the next shortest path
is found. This process is followed for all links in the system in order to
determine a sense of the criticality of each link to network resiliency. The
analysis is carried out for each O-D pair, and the nodes with greatest change in
travel time are determined to be the most critical. Jaller’s methodology allows
traffic managers to identify critical paths for mitigation purposes before the
occurrence of disaster through careful analysis.

## Changes in Accessibility
Accessibility refers to the ease with which individuals can reach the
destinations that matter to them; this is an abstract idea but one that has been
quantified in numerous ways. @dong2006 provide a helpful framework for
understanding various quantitative definitions of accessibility that we will
simplify here. The most elementary definition of accessibility is whether a
destination is within an *isochrone*, or certain distance. This measure is often
represented as a count, e.g., the number of jobs reachable from a particular
location within thirty minutes travel time by a particular mode. Mathematically,

\begin{equation}
A_i = \sum_{j} X_j I_{ij};  I_{ij} = \begin{cases}  1 \text{ if } d_{ij} \leq D\\ 
0 \text{ if } d_{ij} > D \end{cases}
  (\#eq:isochrone)
\end{equation}


where the accessibility A at point i is the sum of the all the jobs or other
destinations $X$ at other points $j$. $I_{ij}$ is an indicator function equal to
zero if the distance between the points $d{_ij}$ is less than some asserted
threshold (e.g., thirty minutes of travel time). By relaxing the assumption of a
binary isochrone and instead using the distance directly, we can derive the
so-called gravity model,

\begin{equation}
A_i = \sum_{j} X_j f(d_{ij})
  (\#eq:gravity)
\end{equation}

where the function $f(d_{ij})$ is often a negative exponential with a calibrated
impedance coefficient. An extension of the gravity model is to use the logsum
term of a multinomial logit destination choice model,

\begin{equation}
A_i = ln\sum_{j} \beta_d(d_{ij}) + X_j\beta
  (\#eq:logsum)
\end{equation}

Where the parameters $\beta$ are estimated from choice surveys or calibrated to
observed data. The logsum term has numerous benefits outlined by [@handy1997]
and [@xiangdong2007]; namely, the measure is based in actual choice theory, and
can include multiple destination types and travel times by multiple different
modes. Accessibility measures of any kind are important in resiliency analysis
because a damaged transport network will limit the ability of people to access
the full variety of destinations they otherwise would.

@guers2004 provide a review of accessibility measures such as
those above up to 2004. Of the papers they reviewed, Vickerman (1974), Ben-Akiva
and Lerman (1979), Geurs and Ritsema van Eck (2001) used isochrone type methods,
Stewart (1947), Hansen (1959), Ingram (1971), and Vickerman (1971), and Anas
(1983) used gravity, and Neuburger (1971), Leonardi (1987), Williams and Senior
(1978), Koenig( 1980), Anas (1983), Ben-Akiva and Lerman (1985), Sweet (1997),
Niemier (1997), Handy and Niemier (1997), Levine (1998), and Miller (1999) used
or suggested logsums. They highlight the importance of using person-based
measures such as these in evaluating network vulnerability and resiliency.

[@taylor2008] also examines accessibility as a way to analyze network
vulnerabilities, mainly through logsum analysis. Taylor begins by explaining the
concepts of vulnerability and accessibility, which further help to explain their
interrelatedness and key distinctions. He then applies logsum analysis to
provide a framework for solving problems of accessibility. Taylor’s method for
identifying vulnerabilities involves modeling travel demand, network topology,
capacity, and road geometry in a manner that closely resembles a graph network.
He does make one key distinction between links and nodes, however. A Node is
vulnerable if link loss to that node diminishes its accessibility, while a Link
is critical if its loss significantly diminishes accessibility within the
network. One drawback to Taylor’s methodology at this point, is that it is
intended to be used after critical locations have been identified.

## Summary
The lessons learned from the events in Minneapolis and Atlanta demonstrate that
when transportation networks are damaged or degraded by link failure, multiple
changes result. Traffic diverts to other facilities and other modes, and some
people make fundamental changes to their daily activity patterns, choosing new
destinations or eliminating trips entirely. Numerous other researchers have
identified methodologies to capture the effects, or at least the costs, of these
potential changes in modeled crisis events. We are able to learn from past and
current methodologies to create a functional methodology on a state-wide level.