# Literature {#litreview}

In a groundbreaking theoretical article, @berdica2002 attempted to identify,
define and conceptualize network "vulnerability" --- the complement of 
resiliency --- by envisioning analyses conducted with
several vulnerability performance measures including travel time, delay,
congestion, serviceability, and accessibility. She then defined reliability as
the level of reduced accessibility due to unfavorable operating conditions on
the network. In particular, Berdica identified a need for further research
toward developing a framework capable of investigating reliability of
transportation networks. 

In this section we examine several attempts by numerous researchers to do
precisely this using various measures of network performance. It is helpful to
categorize this review into three groups based on the overall technique applied 
in the study:

-	*Network connectivity*: How does damage to a network diminish the connectivity 
between network nodes?

-	*Travel time analysis*: How much do shortest path travel times between origins 
and destinations increase on a damaged network?

-	*Accessibility analysis* How easily can the population using the damaged
network complete their daily activities?

The following sections discuss relevant studies in each group; Table
\@ref(tab:authortable) consolidates these studies by year and labels them with
an applicable group.

```{r authortable, echo = FALSE}
read_xlsx("images/author_summary.xlsx") %>%
  kbl(caption = "Attempts to Evaluate Systemic Resiliency", booktabs = TRUE) %>%
  kable_styling()
```

## Network Connectivity
The purpose of transportation networks is to connect locations to each other; 
presumably damage to a network would diminish the network's *connectivity*, or 
the number of paths between node pairs. It may even leave nodes or groups of nodes
completely isolated. In studies based on connectivity, researchers typically apply
methods and concepts from graph theory.

@abdel2007 developed a multi-layered graph to examine the resiliency of the traffic
signal control system in Boise, Idaho. The researchers determined which 
traffic signals would be isolated by a failure to a particular power substation,
and consequentially the percent of travel paths that would experience diminished 
levels of service. The research highlights the degree to which interrelated
infrastructure systems --- power, telecommunications, and transportation ---
depend on each other, though the researchers did not attempt to look at the
connective resiliency of the transportation network directly.

@agarwal2011 present a method to represent a transportation network as a 
hierarchical graph that can be analyzed more directly for vulnerabilities. The
authors acknowledge, however, that a maximal failure consideration where a node
is entirely isolated from the network is unlikely in a real-world network with 
multiple paths of connectivity.

@ip2011 address this shortcoming through the concept of *friability*, or
the reduction of capacity caused by removing a link or node, in order to
determine criticality of individual links. The methodology relies on the ability
to determine the weighted sum of the resilience of all nodes based on the
weighted average of connectedness with other city nodes in the network. The
authors determine that the recovery of transportability between two cities
largely depends on redundant links between nodes. The authors also comment that
most traffic managers are more concerned with the friability of single links
rather than the friability of multiple links or an entire system.

@vodak2019 develop an approach to identify
critical links in a network by searching for the shortest independent
loops in the network. The algorithm progressively damages one or more links
between iterations to determine if nodes become isolated, or cut off from the
network. If a node becomes easily isolated or has a higher likelihood of
becoming isolated, then there is a higher degree of vulnerability present in the
network. This method can both identify critical links in individual networks, as
well as provide a means to quantitatively compare networks.

All of these graph theoretical approaches tend to break down to some degree on
large, real-world networks where the number of nodes and links numbers in the tens
of thousands, and the degree of connectivity between any arbitrary node pair is
high.

## Changes in Travel Time
Highway system network failures --- in most imaginable cases --- degrade the
shortest or least cost path but typically do not eliminate it entirely. The degree
to which travel time increases when a particular link is damaged could provide an
estimate of the criticality of that link.

@peeta2010 construct a model to efficiently allocate
highway maintenance resources. Each link in the sample network was
assigned a specific failure probability based on resource allocation; 
the model evaluated the increase in travel time resulting from a broken
link. A Monte Carlo simulation revealed which allocation plan resulted in the least
network degradation, and thus which links were most critical to the network's 
operations. 

@ibrahim2011 provide an alternative heuristic approach for determining
vulnerability of infrastructure by estimating the cost of single link failure
based on the increase in shortest path travel time due to increased congestion
levels. The authors propose a hybrid heuristic approach that calculates the
traditional user-equilibrium assignment for finding the first set of costs, and
then fixes those costs for all following iterations to determine the effects of
failure on overall travel time of the system.  @omer2013 apply a similar methodology 
to a real-life intercity highway network. @jaller2015 extended this methodology 
with a static user equilibrium traffic loading step to provide an estimate of
how the next-shortest path changes when congested.

A primary limitation with increased travel time methodologies is that they
ignore the other possible ways a population might adapt its travel to a damaged
network. Some people may choose other modes or destinations, and it is possible
that some previously occurring trips might be canceled entirely.

## Changes in Accessibility
A 


Accessibility refers to the ease with which individuals can reach the
destinations that matter to them; this is an abstract idea but one that has been
quantified in numerous ways. @dong2006 provide a helpful framework for
understanding various quantitative definitions of accessibility that we will
simplify here. The most elementary definition of accessibility is whether a
destination is within an *isochrone*, or certain distance. This measure is often
represented as a count, e.g., the number of jobs reachable from a particular
location within thirty minutes travel time by a particular mode. Mathematically,

\begin{equation}
A_i = \sum_{j} X_j I_{ij};  I_{ij} = \begin{cases}  1 \text{ if } d_{ij} \leq D\\ 
0 \text{ if } d_{ij} > D \end{cases}
  (\#eq:isochrone)
\end{equation}

where the accessibility A at point $i$ is the sum of the all the jobs or other
destinations $X$ at other points $j$. $I_{ij}$ is an indicator function equal to
zero if the distance between the points $d{_ij}$ is less than some asserted
threshold (e.g., thirty minutes of travel time). By relaxing the assumption of a
binary isochrone and instead using the distance directly, we can derive the
so-called gravity model,

\begin{equation}
A_i = \sum_{j} X_j f(d_{ij})
  (\#eq:gravity)
\end{equation}

where the function $f(d_{ij})$ is often a negative exponential with a calibrated
impedance coefficient. An extension of the gravity model is to use the logsum
term of a multinomial logit destination choice model,

\begin{equation}
A_i = ln\sum_{j} \beta_d(d_{ij}) + X_j\beta
  (\#eq:logsum)
\end{equation}

Where the parameters $\beta$ are estimated from choice surveys or calibrated to
observed data. The logsum term has numerous benefits outlined by [@handy1997]
and [@xiangdong2007]; namely, the measure is based in actual choice theory, and
can include multiple destination types and travel times by multiple different
modes. 

@guers2004 provide a review of accessibility measures such as
those above up to 2004. Of the papers they reviewed, three used isochrone type methods,
one used gravity, and ten used or suggested logsums in some method, though 
the purpose of this paper was. They
highlight the importance of using person-based measures such as these in
evaluating network vulnerability and resiliency.

@taylor2008 also examines accessibility as a way to analyze network
vulnerabilities, mainly through logsum analysis. Taylor begins by explaining the
concepts of vulnerability and accessibility, which further help to explain their
interrelatedness and key distinctions. He then applies logsum analysis to
provide a framework for solving problems of accessibility. Taylor’s method for
identifying vulnerabilities involves modeling travel demand, network topology,
capacity, and road geometry in a manner that closely resembles a graph network.
He does make one key distinction between links and nodes, however. A Node is
vulnerable if link loss to that node diminishes its accessibility, while a Link
is critical if its loss significantly diminishes accessibility within the
network. One drawback to Taylor’s methodology at this point, is that it is
intended to be used after critical locations have been identified.

## Summary
The lessons learned from the events in Minneapolis and Atlanta demonstrate that
when transportation networks are damaged or degraded by link failure, multiple
changes result. Traffic diverts to other facilities and other modes, and some
people make fundamental changes to their daily activity patterns, choosing new
destinations or eliminating trips entirely. Numerous other researchers have
identified methodologies to capture the effects, or at least the costs, of these
potential changes in modeled crisis events. We are able to learn from past and
current methodologies to create a functional methodology on a state-wide level.